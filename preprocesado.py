import re
import unicodedata
import os
import spacy 
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

BASE_DIR = os.path.dirname(os.path.abspath(__file__))


# ---------------------------------------------------------------------
# 1) EliminaciÃ³n de ruido que NO debe aparecer en el RAG
# ---------------------------------------------------------------------
def limpiar_basura(texto: str) -> str:
    basura_patrones = [
        r"Gaceta PolitÃ©cnica.*?\n",   # encabezados editoriales
        r"www\..*?\n",                # URLs basura
        r"Instituto PolitÃ©cnico Nacional.*?\n",
        r"AÃ‘O.*?VOL\..*?\n",
        r"ISSN.*?\n",
        r"\d{1,3}\s+13 de junio.*?\n",
        r"\d+\s+oiranidroartxE\s+oremÃºN",  # columnas mal pegadas
        r"â€”+", r"-{2,}", r"_{2,}",         # separadores raros
        r"Impreso en.*?\n",
        r"TelÃ©fono.*?\n",
        r"Colaboradores.*?\n",
        r"DiseÃ±o.*?\n",
        r"CoordinaciÃ³n Editorial.*?\n",
        r"Directora General.*?\n",
    ]

    for patron in basura_patrones:
        texto = re.sub(patron, "", texto, flags=re.IGNORECASE)

    return texto


# ---------------------------------------------------------------------
# 2) NormalizaciÃ³n del texto
#    (minus, sin acentos, sin sÃ­mbolos, pero dejando estructura)
# ---------------------------------------------------------------------
def normalizar_simple(texto: str) -> str:
    # MinÃºsculas
    texto = texto.lower()

    # Quitar acentos
    texto = "".join(
        c for c in unicodedata.normalize("NFD", texto)
        if unicodedata.category(c) != "Mn"
    )

    # Reemplazar rarezas
    texto = texto.replace("Âº", "").replace("Â°", "")

    return texto


# ---------------------------------------------------------------------
# 3) IDENTIFICACIÃ“N AUTOMÃTICA
#    - CapÃ­tulos
#    - ArtÃ­culos
#    - ExposiciÃ³n de Motivos
#    - Transitorios
# ---------------------------------------------------------------------
def marcar_secciones(texto: str) -> str:

    # --- ExposiciÃ³n de motivos ---
    texto = re.sub(
        r"exposiciÃ³n de motivos",
        r"# EXPOSICION DE MOTIVOS\nexposicion de motivos",
        texto,
        flags=re.IGNORECASE
    )

    # --- Transitorios ---
    texto = re.sub(
        r"transitorios",
        r"# TRANSITORIOS\ntransitorios",
        texto,
        flags=re.IGNORECASE
    )

    # --- CapÃ­tulos ---
    texto = re.sub(
        r"(cap[iÃ­]tulo\s+[^\n]+)",
        r"# \1",
        texto,
        flags=re.IGNORECASE
    )

    # --- ArtÃ­culos ---
    texto = re.sub(
        r"(art[iÃ­]culo\s+\d+)",
        r"# \1",
        texto,
        flags=re.IGNORECASE
    )

    return texto


# ---------------------------------------------------------------------
# 4) Limpieza final (espacios, lÃ­neas vacÃ­as dobles)
# ---------------------------------------------------------------------
def limpiar_espaciado(texto: str) -> str:
    texto = re.sub(r"\n\s+\n", "\n\n", texto)   # dobles espacios
    texto = re.sub(r"[ ]{2,}", " ", texto)      # dobles espacios en lÃ­nea
    return texto.strip()


# ---------------------------------------------------------------------
# 5) Pipeline completo
# ---------------------------------------------------------------------
def procesar_reglamento(entrada: str, salida_clean: str, salida_final: str):
    print("Leyendo archivo original...")
    with open(entrada, "r", encoding="utf-8") as f:
        texto = f.read()

    print("â†’ Eliminando basura editorial...")
    texto = limpiar_basura(texto)

    print("â†’ Normalizando texto...")
    texto_normalizado = normalizar_simple(texto)

    print("â†’ Guardando texto limpio (sin estructura)...")
    with open(salida_clean, "w", encoding="utf-8") as f:
        f.write(texto_normalizado)

    print("â†’ Detectando capÃ­tulos y artÃ­culos...")
    texto_final = marcar_secciones(texto_normalizado)

    print("â†’ Limpiando espaciado...")
    texto_final = limpiar_espaciado(texto_final)

    print("â†’ Guardando resultado final...")
    with open(salida_final, "w", encoding="utf-8") as f:
        f.write(texto_final)

    print(f"\nâœ” Archivo normalizado creado: {salida_final}\n")
    
def lematizar (texto):
    
    nltk.download("punkt")
    nltk.download("stopwords")
    nltk.download("wordnet")
    nltk.download("punkt_tab") 
    
    texto = texto.lower()
    
    texto = re.sub(r"[^\w\s]", "", texto)
    
    texto = "".join(
    c for c in unicodedata.normalize("NFD", texto)
        if unicodedata.category(c) != "Mn"
    )
    
    tokens = word_tokenize(texto)
    
    stop_words = set(stopwords.words("spanish"))
    tokens = [t for t in tokens if t not in stop_words]
    
    nlp = spacy.load("es_core_news_sm")
    doc = nlp(" ".join(tokens))

    lemmas = [token.lemma_ for token in doc]
    
    return lemmas


# ---------------------------------------------------------------------
# EJECUCIÃ“N DIRECTA (si usas python preprocesado.py)
# ---------------------------------------------------------------------
if __name__ == "__main__":
    archivos = [
        "dictameninfogeneral_extracted.txt",
        "g-extra-1862_extracted.txt"
    ]

    for archivo in archivos:
        entrada = os.path.join(BASE_DIR, archivo)

        salida_clean = entrada.replace("_extracted.txt", "_clean.txt")
        salida_final = entrada.replace("_extracted.txt", "_normalizado.txt")

        print(f"\nðŸ“Œ Procesando {archivo} ...")
        procesar_reglamento(entrada, salida_clean, salida_final)